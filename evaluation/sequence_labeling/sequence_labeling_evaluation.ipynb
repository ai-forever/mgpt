{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b96f8d",
   "metadata": {
    "id": "25f88ea7"
   },
   "source": [
    "# Sequence labeling (NER & POS) evaluation\n",
    "\n",
    "This notebook contains code that reproduces Sequence Labeling mGPT experiments. Namely, this code can be used to evaluate predictions formed by *sequence_labeling_prediction.ipynb*.\n",
    "\n",
    "\n",
    "# Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5f18a",
   "metadata": {
    "id": "GAr0xuDSCG1X"
   },
   "outputs": [],
   "source": [
    "#!pip install seqeval\n",
    "\n",
    "from seqeval.metrics import f1_score, precision_score, accuracy_score, recall_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.precision\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024faa34",
   "metadata": {
    "id": "b5abc930"
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "def calculate_scores(answers, predictions):\n",
    "    langs = answers.keys()\n",
    "    results = []\n",
    "    for l in langs:\n",
    "        results.append([l, accuracy_score(answers[l], predictions[l])])\n",
    "    return results\n",
    "\n",
    "\n",
    "def sequence_general_metrics_(true_label, pred_label):\n",
    "    flat_true_label = []\n",
    "    flat_pred_label = []\n",
    "    for i in range(len(pred_label)):\n",
    "        flat_true_label = flat_true_label + true_label[i]\n",
    "        flat_pred_label = flat_pred_label + pred_label[i]\n",
    "    tag_list = list(sorted(set(flat_true_label)))\n",
    "    random_choice = [random.sample(tag_list, 1) for i in range(len(flat_true_label))]\n",
    "    return [accuracy_score(flat_true_label, flat_pred_label), \\\n",
    "            precision_score(flat_true_label, flat_pred_label, average = 'weighted'), \\\n",
    "            recall_score(flat_true_label, flat_pred_label, average = 'weighted'), \\\n",
    "            f1_score(flat_true_label, flat_pred_label, average = 'weighted'),\n",
    "            precision_score(flat_true_label, random_choice, average = 'weighted'), \\\n",
    "            f1_score(flat_true_label, random_choice, average = 'weighted')]\n",
    "\n",
    "def sequence_general_metrics(true_label, pred_label):\n",
    "    random_choice = []\n",
    "    flat_true_label = []\n",
    "    for i in range(len(true_label)):\n",
    "\n",
    "        for j in range(len(true_label[i])):\n",
    "            if true_label[i][j] == '':\n",
    "                true_label[i][j] = 'X'\n",
    "        flat_true_label = flat_true_label + true_label[i]\n",
    "    tag_list = list(sorted(set(flat_true_label)))\n",
    "    del flat_true_label\n",
    "    for i in range(len(true_label)):\n",
    "        random_choice.append([random.sample(tag_list, 1)[0] for i in range(len(true_label[i]))])\n",
    "\n",
    "    return [accuracy_score(true_label, pred_label), \\\n",
    "            precision_score(true_label, pred_label), \\\n",
    "            recall_score(true_label, pred_label), \\\n",
    "            f1_score(true_label, pred_label),\n",
    "            precision_score(true_label, random_choice), \\\n",
    "            f1_score(true_label, random_choice), \\\n",
    "            accuracy_score(true_label, random_choice)]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_sequence_labeling_scores(answers, predictions):\n",
    "    langs = answers.keys()\n",
    "    results = []\n",
    "    for l in langs:\n",
    "            true = answers[l]\n",
    "            pred = predictions[l]\n",
    "            results.append([l] +  sequence_general_metrics(true, pred))\n",
    "    return pd.DataFrame(results, columns = ['Language', 'Acc', 'Precision', 'Recall', 'F1', 'Random Precision', 'Random F1', 'Random accuracy'])\n",
    "\n",
    "#text preprocessing\n",
    "\n",
    "#regular expression for tags generated by the model (POS-tags, NER-tags)\n",
    "def words_only(text, regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ce355",
   "metadata": {
    "id": "09fdacc5"
   },
   "outputs": [],
   "source": [
    "def show_tag_statistics(true_label, pred_label, lang, task = 'ner'):\n",
    "    if task == 'ner':\n",
    "        tags = {'O', 'I-LOC', 'I-ORG', 'I-PER', 'I-MISC'}\n",
    "    else:\n",
    "        tags =  {'NOUN', 'SCONJ', 'AUX', 'INTJ', 'ADP', 'ADJ', 'PRON', 'DET', 'VERB', 'PUNCT', 'X', 'SYM', 'PART', 'NUM', 'ADV', 'PROPN', 'CCONJ'}\n",
    "\n",
    "    res = []\n",
    "\n",
    "    flat_true_label = []\n",
    "    flat_pred_label = []\n",
    "    for i in range(len(pred_label)):\n",
    "        flat_true_label = flat_true_label + true_label[i]\n",
    "        flat_pred_label = flat_pred_label + pred_label[i]\n",
    "\n",
    "    res_dict = {}\n",
    "    tag_count = Counter()\n",
    "    for key in sorted(tags):\n",
    "        tag_count[key] = 0\n",
    "        res_dict[key] = []\n",
    "\n",
    "    for i, tag in enumerate(flat_true_label):\n",
    "        if tag in tags:\n",
    "            res_dict[tag].append(flat_pred_label[i])\n",
    "            tag_count[tag] += 1\n",
    "    print('Language: ', lang, 'Example number: ', len(flat_true_label))\n",
    "    for key in tag_count.keys():\n",
    "        if len(res_dict[key]):\n",
    "            print(key,'\\tPerc in data:  ', round(tag_count[key]/len(flat_true_label), 3), '   Tag precision:\\t', round(Counter(res_dict[key])[key]/len(res_dict[key]), 3))\n",
    "    print('\\n\\n')\n",
    "    for key in tag_count.keys():\n",
    "        print('Predicted tags for ', key, '(', tag_count[key],'out of',len(flat_true_label),'):')\n",
    "        if tag_count[key] > 0:\n",
    "            for item in Counter(res_dict[key]).most_common(5):\n",
    "                print(item[0],'\\t', item[1],'\\tPerc: ', round(item[1]/tag_count[key], 4))\n",
    "        print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba65aaa",
   "metadata": {
    "id": "b73c9df2"
   },
   "source": [
    "# 4-shot XGLUE NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c673886",
   "metadata": {
    "id": "Zhd4WVGFCa1k"
   },
   "outputs": [],
   "source": [
    "task_name = 'NER_clf'\n",
    "num_examples = 4\n",
    "res4 = pickle.load(open('./'+ task_name +'/pred_few_shot_'+ str(num_examples) +'pred.pkl','rb'))\n",
    "y_true, y_pred = res4[0], res4[1]\n",
    "print('Task name: ', task_name)\n",
    "print('Num few-shot examples: ', num_examples)\n",
    "scores_df = calculate_sequence_labeling_scores(y_true, y_pred).sort_values('Language')\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426de80",
   "metadata": {
    "id": "1fd084a3"
   },
   "source": [
    "# 4-shot XGLUE POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c45994",
   "metadata": {
    "id": "0wM_Tv2QCnzQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "task_name = 'POS_clf'\n",
    "num_examples = 4\n",
    "res4 = pickle.load(open('./'+ task_name +'/pred_few_shot_'+ str(num_examples) +'pred.pkl','rb'))\n",
    "y_true, y_pred = res4[0], res4[1]\n",
    "print('Task name: ', task_name)\n",
    "print('Num few-shot examples: ', num_examples)\n",
    "scores_df = calculate_sequence_labeling_scores(y_true, y_pred).sort_values('Language').reset_index(drop = True)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9739c2",
   "metadata": {
    "id": "4fac6912"
   },
   "source": [
    "# 4-shot CIS & Low resource POS evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7227606",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'UD_POS_clf'\n",
    "num_examples = 4\n",
    "res4 = pickle.load(open('./'+ task_name +'/pred_few_shot_'+ str(num_examples) +'pred.pkl','rb'))\n",
    "y_true, y_pred = res4[0], res4[1]\n",
    "print('Task name: ', task_name)\n",
    "print('Num few-shot examples: ', num_examples)\n",
    "scores_df = calculate_sequence_labeling_scores(y_true, y_pred).sort_values('Language').reset_index(drop = True)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12e0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
