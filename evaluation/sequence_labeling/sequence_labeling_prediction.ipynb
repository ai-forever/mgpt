{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a49c9a8",
   "metadata": {
    "id": "TL4r0laXOISK"
   },
   "source": [
    "# Sequence labeling prediction\n",
    "\n",
    "This notebook contains code that reproduces Sequence Labeling mGPT experiments. Namely, this code can be used to obtain predictions that are further evaluated in *sequence_labeling_evalution.ipynb*.\n",
    "\n",
    "To run the experiments you need to download the data and specify the corresponding data folder as other paths in the configs.\n",
    "\n",
    "XGLUE data can be downloaded from [XGLUE Leaderboard](https://microsoft.github.io/XGLUE/). For this you need to agree to the terms of service. After you do so a download link will be made available.\n",
    "\n",
    "Data for POS evaluation on CIS & Low-Resource UD languages can be found *data/UD_POS_data.tar.gz*. Thus, you need to unpack the file and specify the path to the data directory in `POSUDTaskConfig`. If you work in the original repo, you may simply run the following line of code to extract files in the data folder:\n",
    "\n",
    "`!tar xvzf data/UD_POS_data.tar.gz -C data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33a44c",
   "metadata": {
    "id": "cd445b7a"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Union\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "from inference import load_mgpt, get_dataloader\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f67ff9",
   "metadata": {
    "id": "a0262cfe"
   },
   "source": [
    "# Model load\n",
    "\n",
    "To evaluate $mGPT_{13}$B you need to load $mGPT_{13}$B instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c67b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_mgpt(\"sberbank-ai/mGPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b94af",
   "metadata": {
    "id": "L5wUYzGaOkRU"
   },
   "source": [
    "# Technical functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76ffaa",
   "metadata": {
    "id": "d5b7b171"
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "def calculate_scores(answers, predictions):\n",
    "    langs = answers.keys()\n",
    "    results = []\n",
    "    for l in langs:\n",
    "        results.append([l, accuracy_score(answers[l], predictions[l])])\n",
    "    return results\n",
    "\n",
    "\n",
    "def sequence_general_metrics(true_label, pred_label):\n",
    "    flat_true_label = []\n",
    "    flat_pred_label = []\n",
    "    for i in range(len(pred_label)):\n",
    "        flat_true_label = flat_true_label + true_label[i]\n",
    "        flat_pred_label = flat_pred_label + pred_label[i]\n",
    "    return [precision_score(flat_true_label, flat_pred_label, average = 'weighted'), \\\n",
    "            recall_score(flat_true_label, flat_pred_label, average = 'weighted'), \\\n",
    "            f1_score(flat_true_label, flat_pred_label, average = 'weighted')]\n",
    "\n",
    "\n",
    "def sequence_labeling_em(true_label, pred_label):\n",
    "    ems = []\n",
    "    for idx in range(len(pred_label)):\n",
    "        cur_pred = pred_label[idx]\n",
    "        cur_true = true_label[idx]\n",
    "        if len(cur_pred) != len(cur_true):\n",
    "            print(cur_pred, cur_true)\n",
    "            print('Size mismatch')\n",
    "        else:\n",
    "            cur_res = [1 if cur_pred[i] == cur_true[i] else 0 for i in range(len(cur_pred))]\n",
    "            ems.append(np.mean(cur_res))\n",
    "    return np.mean(ems)\n",
    "\n",
    "def calculate_sequence_labeling_scores(answers, predictions):\n",
    "    langs = answers.keys()\n",
    "    results = []\n",
    "    for l in langs:\n",
    "        true = answers[l]\n",
    "        pred = predictions[l]\n",
    "        results.append([l,  sequence_labeling_em(true, pred)] + sequence_general_metrics(true, pred))\n",
    "    return pd.DataFrame(results, columns = ['Language', 'EM', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "#text preprocessing\n",
    "#regular expression for tags generated by the model (POS-tags, NER-tags)\n",
    "def words_only(text, regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76b7a7",
   "metadata": {
    "id": "OeMPugA_Oopg"
   },
   "source": [
    "# Task configs\n",
    "\n",
    "**Warning:** to run the code you need to indicate the data folder where you store the datasets and specify config directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f17e01",
   "metadata": {
    "id": "47337643"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TaskConfig:\n",
    "    split: str = 'test'\n",
    "    cache_dir: str = './data/'\n",
    "    output_dir: str = './'\n",
    "    save_perplexities: bool = False\n",
    "    save_predictions: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class POSTaskConfig(TaskConfig):\n",
    "    task_name: str = 'POS_clf'\n",
    "    #here you need to write our own path to the dataset\n",
    "    data_dir: str = './data/xglue_full_dataset/POS/'\n",
    "    pred_dir: str = './' \n",
    "    num_examples: int = 4\n",
    "    train_lang = 'en'\n",
    "    #prompts = {0: \"<s>lang: \", 1: \"\\nSentence: \", 2: ' Parts of speech: ', 3:'</s>'}\n",
    "    prompts = {0: \"<s>lang: \", 1: \"\\nTagged sentence: \", 2: '</s>'}\n",
    "    langs = ['de', 'el', 'en', 'es', 'fr','hi', 'it', 'nl','pl', 'pt', 'ru', 'th', 'tr', 'ur', 'vi','ar', 'bg', 'zh']\n",
    "    tag_set =  {'NOUN', 'SCONJ', 'AUX', 'INTJ', 'ADP', 'ADJ', 'PRON', 'DET', 'VERB', 'PUNCT', 'X', 'SYM', 'PART', 'NUM', 'ADV', 'PROPN', 'CCONJ'}\n",
    "\n",
    "    # expression used to filter all but tags from generation result\n",
    "    tags_regex = re.compile(\"[A-Z]+\")\n",
    "    logging = True\n",
    "\n",
    "@dataclass\n",
    "class POSUDTaskConfig(TaskConfig):\n",
    "    task_name: str = 'UD_POS_clf'\n",
    "    #here you need to write our own path to the dataset\n",
    "    data_dir: str = './data/UD/'\n",
    "    #here you need to write the proper path\n",
    "    pred_dir: str = './' \n",
    "    num_examples: int = 0\n",
    "    train_lang = 'en'\n",
    "    prompts = {0: \"<s>lang: \", 1: \"\\nTagged sentence: \", 2: '</s>'}\n",
    "    langs = ['be_hse','uk_iu', 'hy_armtdp', 'kk_ktb', 'bxr_bdt', 'sah_yktdt','tt_nmctt']\n",
    "    tag_set =  {'NOUN', 'SCONJ', 'AUX', 'INTJ', 'ADP', 'ADJ', 'PRON', 'DET', 'VERB', 'PUNCT', 'X', 'SYM', 'PART', 'NUM', 'ADV', 'PROPN', 'CCONJ'}\n",
    "\n",
    "    # expression used to filter all but tags from generation result\n",
    "    tags_regex = re.compile(\"[A-Z]+\")\n",
    "    logging = True\n",
    "\n",
    "@dataclass\n",
    "class NERTaskConfig(TaskConfig):\n",
    "    task_name: str = 'NER_clf'\n",
    "    #here you need to write our own path to the dataset\n",
    "    data_dir: str = './data/xglue_full_dataset/NER/'\n",
    "    #here you need to write the proper path\n",
    "    pred_dir: str = './'\n",
    "    num_examples: int = 4\n",
    "    train_lang = 'en'\n",
    "    langs = ['de', 'en', 'es', 'nl']\n",
    "    prompts = {0: \"<s>lang: \", 1: \"\\nSentence: \", 2: ' Named Entities: ', 3:'</s>'}\n",
    "\n",
    "    # expression used to filter all but tags from generation result\n",
    "    tags_regex = re.compile(\"[A-Z\\-]+\")\n",
    "    logging = True\n",
    "\n",
    "\n",
    "\n",
    "config_pos = POSTaskConfig()\n",
    "config_pos_ud = POSUDTaskConfig(TaskConfig)\n",
    "config_ner = NERTaskConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24a82e",
   "metadata": {
    "id": "da6a0b57"
   },
   "source": [
    "# General prediction class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393c61f",
   "metadata": {
    "id": "ab246a7c"
   },
   "outputs": [],
   "source": [
    "class Task:\n",
    "    def __init__(self, config):\n",
    "        self.prompts = config.prompts\n",
    "        self.langs = config.langs\n",
    "        self.split = config.split\n",
    "        self.cache_dir = config.cache_dir\n",
    "        self.output_dir = config.output_dir\n",
    "        self.task_name = config.task_name\n",
    "        self.save_perplexities = config.save_perplexities\n",
    "        self.save_predictions = config.save_predictions\n",
    "\n",
    "\n",
    "    def verbalize_samples(self, dataset, prompt):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def calculate_scores(self, data, model, batch_size=8):\n",
    "        losses = model.forward(data, loss_per_pos=True, batch_size=batch_size)\n",
    "        return np.asarray([sum(l) for l in losses[0]])\n",
    "\n",
    "    def predict_subset(self, lang, dataset_lang, model):\n",
    "        scores, labels = [], []\n",
    "        for i, prompt in self.prompts.items():\n",
    "            labels.append(i)\n",
    "            text_samples = self.verbalize_samples(dataset_lang, prompt)\n",
    "            print(lang, 'example: \"' + text_samples[0] + '\"')\n",
    "            scores.append(self.calculate_scores(text_samples, model))\n",
    "\n",
    "        scores = np.array(scores).T\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        idx = np.argmin(scores, axis=1)\n",
    "        pred_label = np.take_along_axis(labels, idx, axis=0)\n",
    "        true_label = dataset_lang['label']\n",
    "        scores = np.concatenate((labels.reshape(1, -1), scores))\n",
    "\n",
    "        return true_label, pred_label.tolist(), scores\n",
    "\n",
    "    def predict(self, model):\n",
    "        path_scores = None\n",
    "        dataset = self.load_data()\n",
    "        y_true = {}\n",
    "        y_pred = {}\n",
    "        for i, lang in enumerate(self.langs):\n",
    "            print(i, '/', len(self.langs), ':', lang)\n",
    "            dataset_lang = dataset.filter(lambda example: example['language'] == lang)\n",
    "\n",
    "            true_label, pred_label, scores = self.predict_subset(lang, dataset_lang, model)\n",
    "            y_true[lang] = true_label\n",
    "            y_pred[lang] = pred_label\n",
    "            print('%.2f' % accuracy_score(true_label, pred_label))\n",
    "\n",
    "            if self.save_perplexities:\n",
    "                path_scores = os.path.join(self.output_dir, self.task_name, f\"{lang}_scores.pkl\")\n",
    "                pickle.dump(scores, open(path_scores, 'wb'))\n",
    "\n",
    "        if self.save_predictions:\n",
    "            path = os.path.join(self.output_dir, self.task_name, \"pred.pkl\")\n",
    "            pickle.dump([y_true, y_pred], open(path, 'wb'))\n",
    "        return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad3de01",
   "metadata": {
    "id": "4545d204"
   },
   "source": [
    "# Sequence Labeling Task\n",
    "\n",
    "Universal Sequence Labeling Task solution can be used for both POS and NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23170da1",
   "metadata": {
    "id": "601ef60c"
   },
   "outputs": [],
   "source": [
    "class SequenceLabelingClassificationTask(Task):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.UD_ = False\n",
    "        self.data_dir = config.data_dir\n",
    "        self.prompts = config.prompts\n",
    "        self.num_examples = config.num_examples\n",
    "        #lang for few-shot examples\n",
    "        #in POS and NER tasks few-shot are from English train\n",
    "        self.train_lang = config.train_lang\n",
    "        # expression used to filter all but tags from generation result\n",
    "        self.tags_regex = config.tags_regex\n",
    "        self.logging = config.logging\n",
    "        self.tag_set = {}\n",
    "        self.max_examples = 10000\n",
    "        self.prog = re.compile('\\</?[a-z]+\\>?')\n",
    "\n",
    "        self.print_ = False\n",
    "\n",
    "\n",
    "    #formating few-shot examples\n",
    "    def format_train_data(self, sent):\n",
    "        sent = sent.replace(' ','_').replace('\\n',' ').strip()\n",
    "        result = self.prompts[0] +self.train_lang+ self.prompts[1] +sent+ self.prompts[2]\n",
    "        return result\n",
    "\n",
    "    def format_test_data(self, sent, i, lang, word_res):\n",
    "        if len(word_res) != i:\n",
    "            print('len(word_res) != i')\n",
    "        splitted_sent = sent.replace(' ','_').replace('\\n',' ').strip().split()\n",
    "        splitted_sent = [x.split('_')[0] for x in splitted_sent]\n",
    "        for j in range(i):\n",
    "            splitted_sent[j] = splitted_sent[j] + '_' + word_res[j]\n",
    "        result = self.prompts[0] +lang+ self.prompts[1] +' '.join(splitted_sent[:i]) + ' ' + splitted_sent[i] + '_'\n",
    "        return result\n",
    "\n",
    "    def load_data(self):\n",
    "        #in POS&NER tasks train is available only in English (self.train_lang)\n",
    "        #thus, all the examples for few-shot are in English\n",
    "        train = open(self.data_dir + self.train_lang + '.train', 'r').read().split('\\n\\n')\n",
    "\n",
    "        #delete extra tag markup\n",
    "        train = [x for x in train if not '_ ' in x]\n",
    "        self.tag_set = set([l.split(' ')[1].split('\\n')[0]  for l in train if ' ' in l])\n",
    "        tests = {}\n",
    "        for lang in self.langs:\n",
    "            tests[lang] = [x for x in open(self.data_dir+lang+'.test', 'r').read().split('\\n\\n') if len(x) > 0 and not x == '\\n']\n",
    "        return train, tests\n",
    "\n",
    "    def read_conllu(self, filename):\n",
    "        examples = open(self.data_dir + filename + '-ud-test.conllu','r').read().split('# sent_id ')[1:]\n",
    "        parsed_examples = []\n",
    "        for example in examples:\n",
    "            example = example.split('\\n')\n",
    "            example = [x for x in example if len(x) > 0 and x[0] in '123456789']\n",
    "            example = [x.split('\\t')[1]+' '+x.split('\\t')[3] for x in example]\n",
    "            parsed_examples.append('\\n'.join(example))\n",
    "        return parsed_examples\n",
    "\n",
    "    def load_UD_data(self):\n",
    "        #in POS&NER tasks train is available only in English (self.train_lang)\n",
    "        #thus, all the examples for few-shot are in English\n",
    "        train = open(self.data_dir + self.train_lang + '.train', 'r').read()[1:].split('\\n\\n')\n",
    "\n",
    "        #delete extra tag markup\n",
    "        train = [x for x in train if not '_ ' in x]\n",
    "        self.tag_set = set([l.split(' ')[1].split('\\n')[0]  for l in train if ' ' in l])\n",
    "\n",
    "\n",
    "        tests = {}\n",
    "        for lang in self.langs:\n",
    "            tests[lang] = self.read_conllu(lang)\n",
    "        return train, tests\n",
    "\n",
    "\n",
    "    def classify_with_examples(self, prompt, examples, model, lang='en', top_k = 1, top_p = 0.90, seed = 1337):\n",
    "        if self.num_examples > 0:\n",
    "            #few_shot\n",
    "            some = random.sample(examples, self.num_examples)\n",
    "\n",
    "            #in POS&NER tasks task train is available only in English (self.train_lang)\n",
    "            #thus, all the examples for few-shot are in English\n",
    "            train_examples = [self.format_train_data(s) for s in some]\n",
    "            text = '\\n'.join(train_examples)+'\\n'\n",
    "\n",
    "        else:\n",
    "            #zero-shot\n",
    "            text = ''\n",
    "        #sentence splitted into word_TAG\n",
    "        tagged_sent = [x for x in prompt.replace(' ','_').replace('\\n',' ').strip().split() if '_' in x]\n",
    "        true_tags = [x.split('_')[1] for x in tagged_sent]\n",
    "\n",
    "        word_num = len(true_tags)\n",
    "\n",
    "        if self.print_:\n",
    "            print('Word_num: ', word_num, true_tags)\n",
    "        word_res = []\n",
    "\n",
    "        # word by word generation\n",
    "        tags = sorted(self.tag_set)\n",
    "        for i in range(word_num):\n",
    "            #example Tagged sentence: What_PRON if_SCONJ Google_\n",
    "            test_prompt = example.format_test_data(prompt, i, lang, word_res)\n",
    "            tagged_candidates = [text + test_prompt + tag for tag in tags]\n",
    "            scores = self.calculate_scores(tagged_candidates, model)\n",
    "            word_res.append(tags[np.argmin(scores)])\n",
    "\n",
    "        return true_tags, word_res\n",
    "\n",
    "\n",
    "    def predict_lang(self, dataset_lang, train, lang, model):\n",
    "        true_label = []\n",
    "        pred_label = []\n",
    "\n",
    "        if self.logging:\n",
    "            if not os.path.exists(os.path.join(self.output_dir, self.task_name)):\n",
    "                os.makedirs(os.path.join(self.output_dir, self.task_name))\n",
    "            path = os.path.join(self.output_dir, self.task_name, str(self.num_examples) + \"logs.txt\")\n",
    "            with open(path, 'a') as f:\n",
    "                f.write('Processing language: ' + lang + '\\n')\n",
    "\n",
    "        for i in tqdm(range(min(self.max_examples, len(dataset_lang)))):\n",
    "            true, gen = self.classify_with_examples(dataset_lang[i], train, model, lang=lang)\n",
    "            true_label.append(true)\n",
    "            pred_label.append(gen)\n",
    "            if self.logging:\n",
    "                if not os.path.exists(os.path.join(self.output_dir, self.task_name)):\n",
    "                    os.makedirs(os.path.join(self.output_dir, self.task_name))\n",
    "                if i%10 == 0:\n",
    "                    path = os.path.join(self.output_dir, self.task_name, str(self.num_examples) + \"logs.txt\")\n",
    "                    with open(path, 'a') as f:\n",
    "                        f.write(lang + '\\t' + str(i+1) + ' out of ' + str(min(self.max_examples, len(dataset_lang))) + '\\n')\n",
    "\n",
    "\n",
    "        return true_label, pred_label\n",
    "\n",
    "\n",
    "    def predict(self, model):\n",
    "        if self.UD_:\n",
    "            train, tests = self.load_UD_data()\n",
    "        else:\n",
    "            train, tests = self.load_data()\n",
    "        y_true = {}\n",
    "        y_pred = {}\n",
    "        if self.logging:\n",
    "            if not os.path.exists(os.path.join(self.output_dir, self.task_name)):\n",
    "                os.makedirs(os.path.join(self.output_dir, self.task_name))\n",
    "            path = os.path.join(self.output_dir, self.task_name, str(self.num_examples) + \"logs.txt\")\n",
    "            with open(path, 'w') as f:\n",
    "                f.write('Start processing '+ self.task_name + '\\n')\n",
    "        for lang in tqdm(self.langs):\n",
    "            dataset_lang = tests[lang]\n",
    "            true_label, pred_label = self.predict_lang(dataset_lang, train, lang, model)\n",
    "            y_true[lang] = true_label\n",
    "            y_pred[lang] = pred_label\n",
    "            if self.save_predictions:\n",
    "                path = os.path.join(self.output_dir, self.task_name, \"pred_few_shot_\"+str(self.num_examples)+\"pred.pkl\")\n",
    "                pickle.dump([y_true, y_pred], open(path, 'wb'))\n",
    "        if self.save_predictions:\n",
    "            if not os.path.exists(os.path.join(self.output_dir, self.task_name)):\n",
    "                os.makedirs(os.path.join(self.output_dir, self.task_name))\n",
    "            path = os.path.join(self.output_dir, self.task_name, \"pred_few_shot_\"+str(self.num_examples)+\"pred.pkl\")\n",
    "            pickle.dump([y_true, y_pred], open(path, 'wb'))\n",
    "        return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92556575",
   "metadata": {
    "id": "70a7f6e7"
   },
   "source": [
    "# 4-shot XGLUE NER prediction\n",
    "\n",
    "XGLUE data can be downloaded from [XGLUE Leaderboard](https://microsoft.github.io/XGLUE/). For this you need to agree to the terms of service. After you do so a download link will be made available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc859a0",
   "metadata": {
    "id": "a84e3423",
    "outputId": "a51c0de3-a4a6-4e76-a631-adeb9e48715a"
   },
   "outputs": [],
   "source": [
    "example = SequenceLabelingClassificationTask(config_ner)\n",
    "example.save_predictions = True\n",
    "LANGS = config_ner.langs\n",
    "example.langs = LANGS\n",
    "\n",
    "example.num_examples = 4\n",
    "\n",
    "train, tests = example.load_data()\n",
    "print(example.tag_set)\n",
    "for key in tests.keys():\n",
    "    print(key, len(tests[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99acbc18",
   "metadata": {
    "id": "N9Z3d7t2PS2y"
   },
   "outputs": [],
   "source": [
    "y_true, y_pred = example.predict(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354479c",
   "metadata": {
    "id": "z72qoxl_PJyo"
   },
   "source": [
    "# 4-shot XGLUE POS predictions\n",
    "\n",
    "XGLUE data can be downloaded from [XGLUE Leaderboard](https://microsoft.github.io/XGLUE/). For this you need to agree to the terms of service. After you do so a download link will be made available on the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51616ef8",
   "metadata": {
    "id": "9af70a3a"
   },
   "outputs": [],
   "source": [
    "example = SequenceLabelingClassificationTask(config_pos)\n",
    "example.save_predictions = True\n",
    "LANGS = config_pos.langs\n",
    "\n",
    "example.langs = LANGS\n",
    "example.num_examples = 4\n",
    "\n",
    "train, tests = example.load_data()\n",
    "print(example.tag_set)\n",
    "for key in tests.keys():\n",
    "    print(key, len(tests[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = example.predict(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af0786",
   "metadata": {
    "id": "psUocpn9QbCS"
   },
   "source": [
    "# 4-shot CIS & Low resource POS predictions\n",
    "\n",
    "Data for POS evaluation on CIS & Low-Resource UD languages can be found *./data/UD_POS_data.tar.gz*. If you work in the original repo folder, uncomment and run the cell below to extract files into the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar xvzf data/UD_POS_data.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = SequenceLabelingClassificationTask(config_pos_ud)\n",
    "example.save_predictions = True\n",
    "example.UD_ = True\n",
    "LANGS = config_pos_ud.langs\n",
    "example.langs = LANGS\n",
    "example.num_examples = 4\n",
    "\n",
    "train, tests = example.load_UD_data()\n",
    "print(example.tag_set)\n",
    "for key in tests.keys():\n",
    "    print(key, len(tests[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddadc673",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = example.predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0940f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
