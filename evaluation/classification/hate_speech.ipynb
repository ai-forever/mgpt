{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98250c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import json\n",
    "import tqdm\n",
    "import operator\n",
    "import random\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HATESPEECH_LANGS = [\n",
    "    \"en\",\n",
    "    \"es\",\n",
    "    \"pt\",\n",
    "    \"pl\",\n",
    "    \"it\"\n",
    "]\n",
    "\n",
    "language_dict = {\n",
    "        \"es\": {\n",
    "        \"prompt\": \"La oración es  \",\n",
    "        \"neg1\": \"normal.\",\n",
    "        \"neg2\": \"común.\",\n",
    "        \"neg3\": \"ok.\",\n",
    "        \"neg4\": \"usual.\",\n",
    "        \"neg5\": \"aceptable.\",\n",
    "        \"pos1\": \"sexista.\",\n",
    "        \"pos2\": \"racista.\",\n",
    "        \"pos3\": \"ofensiva.\",\n",
    "        \"pos4\": \"abusivo.\",\n",
    "        \"pos5\": \"odioso.\",\n",
    "    },\n",
    "        \"it\": {\n",
    "        \"prompt\": \"La frase è \",\n",
    "        \"neg1\": \"normale.\",\n",
    "        \"neg2\": \"comune.\",\n",
    "        \"neg3\": \"ok.\",\n",
    "        \"neg4\": \"solito.\",\n",
    "        \"neg5\": \"accettabile.\",\n",
    "        \"pos1\": \"sessista.\",\n",
    "        \"pos2\": \"razzista.\",\n",
    "        \"pos3\": \"offensivo.\",\n",
    "        \"pos4\": \"abusivo.\",\n",
    "        \"pos5\": \"odioso.\",\n",
    "    },\n",
    "        \"en\": {\n",
    "        \"prompt\": \"The sentence is \",\n",
    "        \"neg1\": \"normal.\",\n",
    "        \"neg2\": \"common.\",\n",
    "        \"neg3\": \"ok.\",\n",
    "        \"neg4\": \"usual.\",\n",
    "        \"neg5\": \"acceptable.\",\n",
    "        \"pos1\": \"sexist.\",\n",
    "        \"pos2\": \"racist.\",\n",
    "        \"pos3\": \"offensive.\",\n",
    "        \"pos4\": \"abusive.\",\n",
    "        \"pos5\": \"hateful.\",\n",
    "    },\n",
    "        \"pt\": {\n",
    "        \"prompt\": \"A sentença é  \",\n",
    "        \"neg1\": \"normal.\",\n",
    "        \"neg2\": \"comum.\",\n",
    "        \"neg3\": \"ok.\",\n",
    "        \"neg4\": \"habitual.\",\n",
    "        \"neg5\": \"aceitável.\",\n",
    "        \"pos1\": \"sexista.\",\n",
    "        \"pos2\": \"racista.\",\n",
    "        \"pos3\": \"ofensiva.\",\n",
    "        \"pos4\": \"abusivo.\",\n",
    "        \"pos5\": \"odioso.\",\n",
    "    },\n",
    "        \"pl\":{\n",
    "        \"prompt\": \"Zdanie brzmi \",\n",
    "        \"neg1\": \"normaln.\",\n",
    "        \"neg2\": \"pospolity.\",\n",
    "        \"neg3\": \"ok.\",\n",
    "        \"neg4\": \"zwykły.\",\n",
    "        \"neg5\": \"do przyjęcia.\",\n",
    "        \"pos1\": \"seksistowski.\",\n",
    "        \"pos2\": \"rasistowski.\",\n",
    "        \"pos3\": \"ofensywa.\",\n",
    "        \"pos4\": \"obraźliwy.\",\n",
    "        \"pos5\": \"nienawistny.\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# PROMPT\n",
    "# The sentence is <candidate>\n",
    "# Candidates negative: normal., common., ok., usual., acceptable.\n",
    "# Candidates positive: sexist., racist., offensive., abusive., hateful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42583800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hatespeech_eval(prompt_n1, prompt_n2, prompt_n3, prompt_n4, prompt_n5, \n",
    "                    prompt_p1, prompt_p2, prompt_p3, prompt_p4, prompt_p5):\n",
    "    ppl1, nll1, scores1 = get_logprobs(prompt_n1)\n",
    "    ppl2, nll2, scores2 = get_logprobs(prompt_n2)\n",
    "    ppl3, nll3, scores3 = get_logprobs(prompt_n3)\n",
    "    ppl4, nll4, scores4 = get_logprobs(prompt_n4)\n",
    "    ppl5, nll5, scores5 = get_logprobs(prompt_n5)\n",
    "    \n",
    "    ppl6, nll6, scores6 = get_logprobs(prompt_p1)\n",
    "    ppl7, nll7, scores7 = get_logprobs(prompt_p2)\n",
    "    ppl8, nll8, scores8 = get_logprobs(prompt_p3)\n",
    "    ppl9, nll9, scores9 = get_logprobs(prompt_p4)\n",
    "    ppl10, nll10, scores10 = get_logprobs(prompt_p5)\n",
    "    \n",
    "    sc_n1, sc_n2, sc_n3, sc_n4, sc_n5 = scores1.sum(), scores2.sum(), scores3.sum(), scores4.sum(), scores5.sum()\n",
    "    sc_p1, sc_p2, sc_p3, sc_p4, sc_p5 = scores6.sum(), scores7.sum(), scores8.sum(), scores9.sum(), scores10.sum()\n",
    "    my_list = [sc_n1, sc_n2, sc_n3, sc_n4, sc_n5, sc_p1, sc_p2, sc_p3, sc_p4, sc_p5]\n",
    "    index, value = max(enumerate(my_list), key=operator.itemgetter(1))\n",
    "    \n",
    "        \n",
    "    neg = [scores1.tolist(), scores2.tolist(), scores3.tolist(), scores4.tolist(), scores5.tolist()]\n",
    "    pos = [scores6.tolist(), scores7.tolist(), scores8.tolist(), scores9.tolist(), scores10.tolist()]\n",
    "\n",
    "    promts = [prompt_n1, prompt_n2, prompt_n3, prompt_n4, prompt_n5, prompt_p1, prompt_p2, prompt_p3, prompt_p4, prompt_p5]\n",
    "\n",
    "        \n",
    "    if index in [1,2,3,4,5]:\n",
    "        predict = 0\n",
    "    else:\n",
    "        predict = 1\n",
    "    return predict, my_list, neg, pos, promts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aea2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logprobs(prompt):\n",
    "    import re\n",
    "    prompt = re.sub(\"\\n+\", \"\\n\", prompt)\n",
    "    \n",
    "    nll = model.forward([prompt], loss_per_pos=False)[0][0]  \n",
    "    ppl = math.exp(nll)\n",
    "    losses = model.forward([prompt], loss_per_pos=True)[0][0]\n",
    "    return (\n",
    "        ppl,\n",
    "        nll,\n",
    "        np.array([loss * -1.0 for loss in losses]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c74c7c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea4b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import load_mgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44f4eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_mgpt(\"sberbank-ai/mGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/hate_speech_test.json\") as inf:\n",
    "    hatedata = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/hate_speech_train.json\") as inf:\n",
    "    hatedata_train = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd31205",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in language_dict:\n",
    "    print(lang, len(hatedata[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92465057",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in language_dict:\n",
    "    print(lang, len(hatedata[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_df(hatedata, lang, language_dict):\n",
    "\n",
    "    label_dict = {0: ['neg1', 'neg2', 'neg3', 'neg4', 'neg5'],\n",
    "                 1: ['pos1', 'pos2', 'pos3', 'pos4', 'pos5']}\n",
    "\n",
    "    df = pd.DataFrame(hatedata[lang], columns=['label', 'text'])\n",
    "    samples = []\n",
    "    for row in df.values:\n",
    "        labels = label_dict[row[0]]\n",
    "        sample = np.random.choice(labels)\n",
    "        prompt = \"{}{} {}\".format(\n",
    "            language_dict[lang][\"prompt\"],\n",
    "            language_dict[lang][sample],\n",
    "            row[1]\n",
    "        )\n",
    "        samples.append(prompt)\n",
    "    df['prompt'] = samples\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_data(hatedata, lang, language_dict):\n",
    "    label_dict = {0: ['neg1', 'neg2', 'neg3', 'neg4', 'neg5'],\n",
    "                 1: ['pos1', 'pos2', 'pos3', 'pos4', 'pos5']}\n",
    "\n",
    "    train_data = []\n",
    "    for row in hatedata[lang]:\n",
    "        labels = label_dict[row[0]]\n",
    "        sample = np.random.choice(labels)\n",
    "        prompt = \"{}{} {}\".format(\n",
    "            language_dict[lang][\"prompt\"],\n",
    "            language_dict[lang][sample],\n",
    "            row[1]\n",
    "        )\n",
    "        train_data.append(prompt)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGS = ['pt', 'pl', 'es', 'it', 'en']\n",
    "num_shots = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87d25e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "for lang in LANGS:\n",
    "    if lang not in all_results:\n",
    "        all_results[lang] = {'gold':[], 'pred':[]}\n",
    "    results_pred = []\n",
    "    results_gold = []\n",
    "    print(lang)\n",
    "\n",
    "    train_data = preprocess_train_data(hatedata_train, lang, language_dict)\n",
    "    \n",
    "    scores = []\n",
    "    neg_scores = []\n",
    "    pos_scores = []\n",
    "    promts_list = []\n",
    "    for idx, row in enumerate(tqdm.tqdm(hatedata[lang])):\n",
    "\n",
    "        if num_shots:\n",
    "            sample = np.random.choice(train_data, num_shots)\n",
    "            shots = '\\n'.join(sample) + '\\n'\n",
    "        else:\n",
    "            shots = ''\n",
    "        \n",
    "        propmpts_n = []\n",
    "        for i in range(1,6):\n",
    "            prompt = \"{}{}{} {}\".format(\n",
    "                shots,\n",
    "                language_dict[lang][\"prompt\"],\n",
    "                language_dict[lang][\"neg{}\".format(i)],\n",
    "                row[1]\n",
    "            )\n",
    "            propmpts_n.append(prompt)\n",
    "\n",
    "        propmpts_p = []\n",
    "        for i in range(1,6):\n",
    "            prompt = \"{}{}{} {}\".format(\n",
    "                shots,\n",
    "                language_dict[lang][\"prompt\"],\n",
    "                language_dict[lang][\"pos{}\".format(i)],\n",
    "                row[1]\n",
    "            )\n",
    "            propmpts_p.append(prompt)\n",
    "\n",
    "        prompt_n1, prompt_n2, prompt_n3, prompt_n4, prompt_n5 = propmpts_n\n",
    "        prompt_p1, prompt_p2, prompt_p3, prompt_p4, prompt_p5 = propmpts_p\n",
    "\n",
    "        label_gold = row[0]\n",
    "        predict, my_list, neg, pos, promts = hatespeech_eval(\n",
    "            prompt_n1, prompt_n2, prompt_n3, prompt_n4, prompt_n5, \n",
    "            prompt_p1, prompt_p2, prompt_p3, prompt_p4, prompt_p5\n",
    "        )\n",
    "        results_pred.append(str(predict))\n",
    "        results_gold.append(str(label_gold))\n",
    "    \n",
    "        scores.append(my_list)\n",
    "        neg_scores.append(neg)\n",
    "        pos_scores.append(pos)\n",
    "        promts_list.append(promts)\n",
    "    \n",
    "    all_results[lang]['gold'] = results_gold\n",
    "    all_results[lang]['pred'] = results_pred\n",
    "\n",
    "    accuracy = accuracy_score(results_gold, results_pred)\n",
    "    pr_ma, rc_ma, f1_ma, _ = precision_recall_fscore_support(results_gold, results_pred, average='macro')\n",
    "    pr_mi, rc_mi, f1_mi, _ = precision_recall_fscore_support(results_gold, results_pred, average='micro')\n",
    "    pr_w, rc_w, f1_w, _ = precision_recall_fscore_support(results_gold, results_pred, average='weighted')\n",
    "    print(lang)\n",
    "    print(accuracy, len(results_gold))\n",
    "    print(pr_ma, rc_ma, f1_ma)\n",
    "    print(pr_mi, rc_mi, f1_mi)\n",
    "    print(pr_w, rc_w, f1_w)\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea66502",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang, vals in all_results.items():\n",
    "    results_gold = vals['gold']\n",
    "    results_pred = vals['pred']\n",
    "    accuracy = accuracy_score(results_gold, results_pred)\n",
    "    pr_ma, rc_ma, f1_ma, _ = precision_recall_fscore_support(results_gold, results_pred, average='macro')\n",
    "    pr_mi, rc_mi, f1_mi, _ = precision_recall_fscore_support(results_gold, results_pred, average='micro')\n",
    "    pr_w, rc_w, f1_w, _ = precision_recall_fscore_support(results_gold, results_pred, average='weighted')\n",
    "    print(lang)\n",
    "    print(accuracy, len(results_gold))\n",
    "    print(pr_ma, rc_ma, f1_ma)\n",
    "    print(pr_mi, rc_mi, f1_mi)\n",
    "    print(pr_w, rc_w, f1_w)\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1ba14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a37d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c17bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
